{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a46a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "detals_filename ='names.txt'\n",
    "\n",
    "details =open(detals_filename, mode=\"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "details[0]= \"SOSNA\"+\"\\n\"\n",
    "details[1]= \"MEZENCWEVA\"+\"\\n\"\n",
    "details[2] = \"Vetupravlenie\"+\"\\n\"\n",
    "\n",
    "str(details)\n",
    "\n",
    "file= open(detals_filename, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "str(details)\n",
    "#file.write(str(details))\n",
    "\n",
    "string_to_write = str()\n",
    "string_to_write= string_to_write+details[0]\n",
    "string_to_write= string_to_write+details[1]\n",
    "string_to_write= string_to_write+details[2]\n",
    "file.write(string_to_write)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d07608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def merge_sqlite_databases(db1_path, db2_path, output_db_path, conflict_strategy='ignore'):\n",
    "    \"\"\"\n",
    "    Объединяет две базы данных SQLite с одинаковой структурой таблиц\n",
    "    в одну базу данных с обработкой конфликтов по первичным ключам.\n",
    "\n",
    "    Parameters:\n",
    "    db1_path (str): Путь к первой базе данных\n",
    "    db2_path (str): Путь ко второй базе данных\n",
    "    output_db_path (str): Путь для сохранения объединённой базы данных\n",
    "    conflict_strategy (str): Стратегия обработки конфликтов ('ignore' или 'replace')\n",
    "    \"\"\"\n",
    "    if conflict_strategy == 'ignore':\n",
    "        insert_clause = 'INSERT OR IGNORE'\n",
    "    elif conflict_strategy == 'replace':\n",
    "        insert_clause = 'INSERT OR REPLACE'\n",
    "    else:\n",
    "        raise ValueError(\"conflict_strategy должен быть 'ignore' или 'replace'\")\n",
    "\n",
    "    # Копируем первую БД в выходной файл\n",
    "    if os.path.exists(output_db_path):\n",
    "        os.remove(output_db_path)\n",
    "    with open(db1_path, 'rb') as f_src, open(output_db_path, 'wb') as f_dst:\n",
    "        f_dst.write(f_src.read())\n",
    "\n",
    "    conn_out = sqlite3.connect(output_db_path)\n",
    "    cursor_out = conn_out.cursor()\n",
    "\n",
    "    # Подключаем вторую базу как ATTACH\n",
    "    cursor_out.execute(f\"ATTACH DATABASE '{db2_path}' AS db2\")\n",
    "\n",
    "    # Получаем список таблиц в основной базе (output),\n",
    "    # игнорируем служебные\n",
    "    cursor_out.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
    "    tables = [row[0] for row in cursor_out.fetchall()]\n",
    "\n",
    "    for table in tables:\n",
    "        sql = f'{insert_clause} INTO {table} SELECT * FROM db2.{table}'\n",
    "        try:\n",
    "            cursor_out.execute(sql)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при вставке данных в таблицу {table}: {e}\")\n",
    "\n",
    "    conn_out.commit()\n",
    "    cursor_out.execute(\"DETACH DATABASE db2\")\n",
    "    conn_out.close()\n",
    "\n",
    "    print(f\"Объединение завершено. Итоговая база: {output_db_path}\")\n",
    "\n",
    "# Пример вызова:\n",
    "# merge_sqlite_databases('db1.sqlite', 'db2.sqlite', 'merged_db.sqlite', conflict_strategy='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92de9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка вставки в таблицу gay_clubs: UNIQUE constraint failed: gay_clubs.pk\n",
      "Ошибка вставки в таблицу porno: UNIQUE constraint failed: porno.pk\n",
      "Объединение завершено. Итоговая база: result_database.sqlite3\n"
     ]
    }
   ],
   "source": [
    "db1_path='./nichego/baza1.sqlite3'\n",
    "db2_path='./nichego/baza2.sqlite3'\n",
    "output_db_path = 'result_database.sqlite3'\n",
    "merge_sqlite_databases(db1_path, db2_path, output_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52367cf4-ab43-4028-90e2-e744fe6755a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import base64\n",
    "import os\n",
    "import secrets\n",
    "\n",
    "def load_dbs_as_dfs(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Получаем список таблиц\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    dfs = {}\n",
    "    for table in tables:\n",
    "        dfs[table] = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    conn.close()\n",
    "    return dfs\n",
    "\n",
    "def generate_256bit_base64():\n",
    "    rand_bytes = secrets.token_bytes(32)  # 256 бит = 32 байта\n",
    "    return base64.urlsafe_b64encode(rand_bytes).rstrip(b'=').decode('ascii')\n",
    "\n",
    "def replace_keys_base64(dfs, pk_fk_info):\n",
    "    \"\"\"\n",
    "    dfs: dict таблица -> dataframe\n",
    "    pk_fk_info: dict с описанием, где первичные ключи и внешние ключи, вида:\n",
    "      {\n",
    "        'table_name': {\n",
    "           'pk': 'primary_key_column_name',\n",
    "           'fks': { 'fk_column_name': 'referenced_table' }\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    Возвращает новый словарь датафреймов с заменёнными ключами в Base64.\n",
    "    \"\"\"\n",
    "    # Словарь для хранения маппинга старых pk -> новых base64 ключей для каждой таблицы\n",
    "    pk_maps = {}\n",
    "\n",
    "    dfs_new = {}\n",
    "\n",
    "    # Сначала для каждой таблицы заменим PK на base64 значения и запомним сопоставление\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        if pk_col is None:\n",
    "            # Нет PK — просто копируем таблицу без изменений\n",
    "            dfs_new[table] = df.copy()\n",
    "            continue\n",
    "\n",
    "        # Создаём мапинг\n",
    "        old_pks = df[pk_col].tolist()\n",
    "        new_pks = [generate_256bit_base64() for _ in old_pks]\n",
    "\n",
    "        pk_map = dict(zip(old_pks, new_pks))\n",
    "        pk_maps[table] = pk_map\n",
    "\n",
    "        df_copy = df.copy()\n",
    "        df_copy[pk_col] = df_copy[pk_col].map(pk_map)\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    # Теперь обновим внешние ключи, если они есть\n",
    "    for table, df in dfs_new.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        fks = info.get('fks', {})\n",
    "        if not fks:\n",
    "            continue\n",
    "\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        for fk_col, ref_table in fks.items():\n",
    "            if fk_col not in df_copy.columns:\n",
    "                continue\n",
    "            if ref_table not in pk_maps:\n",
    "                continue\n",
    "            fk_map = pk_maps[ref_table]\n",
    "            # Для значений внешнего ключа, которые есть в fk_map, меняем, а если нет — оставляем как есть\n",
    "            df_copy[fk_col] = df_copy[fk_col].map(lambda x: fk_map.get(x, x))\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    return dfs_new\n",
    "\n",
    "def merge_dfs(dfs1, dfs2):\n",
    "    \"\"\"\n",
    "    Объединяем две словаря таблиц (dataframe) по ключу (названию таблиц)\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    all_tables = set(dfs1.keys()).union(dfs2.keys())\n",
    "    for table in all_tables:\n",
    "        df1 = dfs1.get(table)\n",
    "        df2 = dfs2.get(table)\n",
    "        if df1 is not None and df2 is not None:\n",
    "            merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "        elif df1 is not None:\n",
    "            merged_df = df1.copy()\n",
    "        else:\n",
    "            merged_df = df2.copy()\n",
    "        merged[table] = merged_df\n",
    "    return merged\n",
    "\n",
    "def replace_base64_with_autoinc(dfs, pk_fk_info):\n",
    "    \"\"\"\n",
    "    Заменяет base64 ключи в объединённом наборе dfs на autoincrement int ключи,\n",
    "    поправив внешние ключи.\n",
    "\n",
    "    Возвращает новый набор dfs.\n",
    "    \"\"\"\n",
    "    pk_maps = {}  # table_name -> { old_base64_key: new_int_key }\n",
    "\n",
    "    dfs_new = {}\n",
    "\n",
    "    # Сначала для каждой таблицы создаём маппинг base64 pk -> int pk\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        if pk_col is None:\n",
    "            dfs_new[table] = df.copy()\n",
    "            continue\n",
    "\n",
    "        old_pks = df[pk_col].tolist()\n",
    "        # Создаем инт айдишки, нам надо уникальные:\n",
    "        unique_old_pks = pd.Series(old_pks).unique()\n",
    "        pk_map = dict(zip(unique_old_pks, range(1, len(unique_old_pks) + 1)))\n",
    "        pk_maps[table] = pk_map\n",
    "\n",
    "        # Заменяем pk в df\n",
    "        df_copy = df.copy()\n",
    "        df_copy[pk_col] = df_copy[pk_col].map(pk_map)\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    # Обновляем foreign key\n",
    "    for table, df in dfs_new.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        fks = info.get('fks', {})\n",
    "        if not fks:\n",
    "            continue\n",
    "        df_copy = df.copy()\n",
    "        for fk_col, ref_table in fks.items():\n",
    "            if fk_col not in df_copy.columns:\n",
    "                continue\n",
    "            if ref_table not in pk_maps:\n",
    "                continue\n",
    "            fk_map = pk_maps[ref_table]\n",
    "            df_copy[fk_col] = df_copy[fk_col].map(lambda x: fk_map.get(x, x))\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    return dfs_new\n",
    "\n",
    "def write_dfs_to_sqlite(dfs, pk_fk_info, output_db_path):\n",
    "    \"\"\"\n",
    "    Записывает датафреймы в SQLite файл\n",
    "\n",
    "    При этом для таблиц с PK ставит INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    для других просто создаёт таблицы.\n",
    "\n",
    "    !! Внимание !! \n",
    "    Здесь упрощённая логика создания таблиц — для реальной сложной схемы \n",
    "    стоит использовать инспектор схемы или ORM\n",
    "\n",
    "    Принимает:\n",
    "      - dfs: dict table->DataFrame\n",
    "      - pk_fk_info: dict с info по key-ам\n",
    "      - output_db_path: файл для записи\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_db_path):\n",
    "        os.remove(output_db_path)\n",
    "\n",
    "    conn = sqlite3.connect(output_db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        fks = info.get('fks', {})\n",
    "\n",
    "        # Формируем схему\n",
    "        columns = df.columns.tolist()\n",
    "        col_defs = []\n",
    "\n",
    "        for col in columns:\n",
    "            dtype = df[col].dtype\n",
    "            if pd.api.types.is_integer_dtype(dtype):\n",
    "                col_type = 'INTEGER'\n",
    "            elif pd.api.types.is_float_dtype(dtype):\n",
    "                col_type = 'REAL'\n",
    "            else:\n",
    "                col_type = 'TEXT'\n",
    "\n",
    "            if pk_col == col:\n",
    "                col_def = f\"{col} INTEGER PRIMARY KEY AUTOINCREMENT\"\n",
    "            else:\n",
    "                col_def = col + ' ' + col_type\n",
    "            col_defs.append(col_def)\n",
    "\n",
    "        sql_create = f\"CREATE TABLE {table} ({', '.join(col_defs)}\"\n",
    "\n",
    "        # Добавляем внешние ключи (если есть)\n",
    "        if fks:\n",
    "            fk_defs = []\n",
    "            for fk_col, ref_table in fks.items():\n",
    "                pk_ref_col = pk_fk_info[ref_table]['pk']\n",
    "                fk_defs.append(f\"FOREIGN KEY ({fk_col}) REFERENCES {ref_table}({pk_ref_col})\")\n",
    "            if fk_defs:\n",
    "                sql_create += ', ' + ', '.join(fk_defs)\n",
    "\n",
    "        sql_create += ');'\n",
    "\n",
    "        cursor.execute(sql_create)\n",
    "\n",
    "        # Вставляем данные\n",
    "        placeholders = ', '.join(['?' for _ in columns])\n",
    "        insert_sql = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "        values = df.where(pd.notnull(df), None).values.tolist()\n",
    "        cursor.executemany(insert_sql, values)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def merge_sqlite_dbs_with_key_replacement(db1_path, db2_path, output_db_path, pk_fk_info):\n",
    "    \"\"\"\n",
    "    Основная функция:\n",
    "    1) Загружает обе базы в DF\n",
    "    2) Заменяет PK в обеих базах на 256бит base64 ключи с обновлением FK\n",
    "    3) Объединяет DF по таблицам\n",
    "    4) Заменяет base64 ключи на int PK с обновлением FK\n",
    "    5) Записывает в SQLite\n",
    "\n",
    "    pk_fk_info: структура с info по первичным и внешним ключам\n",
    "\n",
    "    Пример pk_fk_info:\n",
    "    {\n",
    "       'authors': {\n",
    "           'pk': 'author_id',\n",
    "           'fks': {}\n",
    "       },\n",
    "       'books': {\n",
    "           'pk': 'book_id',\n",
    "           'fks': {'author_id': 'authors'}\n",
    "       }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Загрузка баз...\")\n",
    "    dfs1 = load_dbs_as_dfs(db1_path)\n",
    "    dfs2 = load_dbs_as_dfs(db2_path)\n",
    "\n",
    "    print(\"Заменяем PK на base64 в первой базе...\")\n",
    "    dfs1_b64 = replace_keys_base64(dfs1, pk_fk_info)\n",
    "\n",
    "    print(\"Заменяем PK на base64 во второй базе...\")\n",
    "    dfs2_b64 = replace_keys_base64(dfs2, pk_fk_info)\n",
    "\n",
    "    print(\"Объединяем датафреймы...\")\n",
    "    merged_dfs = merge_dfs(dfs1_b64, dfs2_b64)\n",
    "\n",
    "    print(\"Заменяем base64 PK на int PK с автоинкрементом...\")\n",
    "    final_dfs = replace_base64_with_autoinc(merged_dfs, pk_fk_info)\n",
    "\n",
    "    print(\"Записываем в SQLite...\")\n",
    "    write_dfs_to_sqlite(final_dfs, pk_fk_info, output_db_path)\n",
    "\n",
    "    print(f\"Обработка завершена. Итоговая база: {output_db_path}\")\n",
    "\n",
    "\n",
    "# --- ПРИМЕР ВЫЗОВА ---\n",
    "# Необходимо задать pk_fk_info для вашей схемы БД (пример ниже)\n",
    "# pk_fk_info = {\n",
    "#   'table1': {'pk': 'id', 'fks': {'parent_id': 'table1'}},\n",
    "#   'table2': {'pk': 'id', 'fks': {'table1_id': 'table1'}},\n",
    "# }\n",
    "\n",
    "# merge_sqlite_dbs_with_key_replacement('db1.sqlite', 'db2.sqlite', 'merged.sqlite', pk_fk_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea4dd6f-a9dd-47b6-83b3-b4e6b6438722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_fk_info = {\n",
    "    'animal_docs': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'animal': 'animals_simple',\n",
    "            'cattle': 'cattle'\n",
    "        }\n",
    "    },\n",
    "    'animals_simple': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'cattle': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'city': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_settlement': 'settlement'\n",
    "        }\n",
    "    },\n",
    "    'conducted_tests': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'animal': 'cattle'\n",
    "        }\n",
    "    },\n",
    "    'household': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_city': 'city'\n",
    "        }\n",
    "    },\n",
    "    'report_entries': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_report': 'reports',\n",
    "            'household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'reports': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'city': 'city'\n",
    "        }\n",
    "    },\n",
    "    'settlement': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2ce44b-8d36-4cfc-a7cd-3065bcaed748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка баз...\n",
      "Заменяем PK на base64 в первой базе...\n",
      "Заменяем PK на base64 во второй базе...\n",
      "Объединяем датафреймы...\n",
      "Заменяем base64 PK на int PK с автоинкрементом...\n",
      "Записываем в SQLite...\n",
      "Обработка завершена. Итоговая база: result_database.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# PRIMER BLIN\n",
    "pk_fk_info = {\n",
    "    'gay_clubs': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {}\n",
    "    },\n",
    "    'porno': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'gay_club': 'gay_clubs'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "merge_sqlite_dbs_with_key_replacement(db1_path, db2_path, output_db_path, pk_fk_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ed67e-5059-4981-aed9-dbf010387545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
