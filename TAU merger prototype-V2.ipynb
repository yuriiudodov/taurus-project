{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a46a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07608c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92de9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_path='./nichego/baza1.sqlite3'\n",
    "db2_path='./nichego/baza2.sqlite3'\n",
    "output_db_path = 'result_database.sqlite3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a152000-a3e2-4d15-99ee-284d7c2e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_merged_dfs(merged_dfs, pk_fk_info):\n",
    "    \"\"\"\n",
    "    Удаляет дубликаты записей в каждой таблице:\n",
    "    - записи считаются дубликатами, если совпадают все столбцы кроме PK\n",
    "    - оставляет первую запись\n",
    "    - обновляет foreign key в других таблицах, чтобы вместо удалённых ключей ссылались на оставшиеся\n",
    "\n",
    "    Возвращает новый словарь датафреймов с устранёнными дубликатами и исправленными FK.\n",
    "    \"\"\"\n",
    "    dfs = {table: df.copy() for table, df in merged_dfs.items()}\n",
    "\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        if pk_col is None:\n",
    "            continue\n",
    "\n",
    "        # Колонки для сравнения (все, кроме PK)\n",
    "        compare_cols = [col for col in df.columns if col != pk_col]\n",
    "        if not compare_cols:\n",
    "            # Все столбцы — PK, пропускаем\n",
    "            continue\n",
    "\n",
    "        # Найдём дубликаты по значимым колонкам, сохранаем первую встреченную\n",
    "        # keep='first' — оставляет первую, остальные отмечает.\n",
    "        duplicate_mask = df.duplicated(subset=compare_cols, keep='first')\n",
    "        if not duplicate_mask.any():\n",
    "            continue\n",
    "\n",
    "        # build map: копии PK -> оригинальный PK (первый встретившийся)\n",
    "        original_pks_map = {}\n",
    "\n",
    "        # для удобства сгруппируем по значимым колонкам\n",
    "        grouped = df.groupby(compare_cols)\n",
    "\n",
    "        for group_vals, group_df in grouped:\n",
    "            # если в группе несколько записей - значит есть дубликаты\n",
    "            if len(group_df) > 1:\n",
    "                original_pk = group_df.iloc[0][pk_col]\n",
    "                duplicate_pks = group_df.iloc[1:][pk_col].tolist()\n",
    "                for dup_pk in duplicate_pks:\n",
    "                    original_pks_map[dup_pk] = original_pk\n",
    "\n",
    "        # Удаляем дубликаты — оставляем только первую запись на группу\n",
    "        dfs[table] = df[~duplicate_mask].copy()\n",
    "\n",
    "        # Для всех foreign keys, которые ссылаются на эту таблицу,\n",
    "        # необходимо заменить FK значения dup_pk на original_pk\n",
    "        for other_table, other_df in dfs.items():\n",
    "            other_info = pk_fk_info.get(other_table, {})\n",
    "            other_fks = other_info.get('fks', {})\n",
    "            for fk_col, ref_table in other_fks.items():\n",
    "                if ref_table == table:\n",
    "                    # Обновим внешние ключи\n",
    "                    # map заменит дубликаты на оригиналы, остальные оставит без изменений\n",
    "                    dfs[other_table][fk_col] = other_df[fk_col].map(lambda x: original_pks_map.get(x, x))\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52367cf4-ab43-4028-90e2-e744fe6755a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import base64\n",
    "import os\n",
    "import secrets\n",
    "\n",
    "def load_dbs_as_dfs(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Получаем список таблиц\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    dfs = {}\n",
    "    for table in tables:\n",
    "        dfs[table] = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    conn.close()\n",
    "    return dfs\n",
    "\n",
    "def generate_256bit_base64():\n",
    "    rand_bytes = secrets.token_bytes(32)  # 256 бит = 32 байта\n",
    "    return base64.urlsafe_b64encode(rand_bytes).rstrip(b'=').decode('ascii')\n",
    "\n",
    "def replace_keys_base64(dfs, pk_fk_info):\n",
    "    \"\"\"\n",
    "    dfs: dict таблица -> dataframe\n",
    "    pk_fk_info: dict с описанием, где первичные ключи и внешние ключи, вида:\n",
    "      {\n",
    "        'table_name': {\n",
    "           'pk': 'primary_key_column_name',\n",
    "           'fks': { 'fk_column_name': 'referenced_table' }\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    Возвращает новый словарь датафреймов с заменёнными ключами в Base64.\n",
    "    \"\"\"\n",
    "    # Словарь для хранения маппинга старых pk -> новых base64 ключей для каждой таблицы\n",
    "    pk_maps = {}\n",
    "\n",
    "    dfs_new = {}\n",
    "\n",
    "    # Сначала для каждой таблицы заменим PK на base64 значения и запомним сопоставление\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        if pk_col is None:\n",
    "            # Нет PK — просто копируем таблицу без изменений\n",
    "            dfs_new[table] = df.copy()\n",
    "            continue\n",
    "\n",
    "        # Создаём мапинг\n",
    "        old_pks = df[pk_col].tolist()\n",
    "        new_pks = [generate_256bit_base64() for _ in old_pks]\n",
    "\n",
    "        pk_map = dict(zip(old_pks, new_pks))\n",
    "        pk_maps[table] = pk_map\n",
    "\n",
    "        df_copy = df.copy()\n",
    "        df_copy[pk_col] = df_copy[pk_col].map(pk_map)\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    # Теперь обновим внешние ключи, если они есть\n",
    "    for table, df in dfs_new.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        fks = info.get('fks', {})\n",
    "        if not fks:\n",
    "            continue\n",
    "\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        for fk_col, ref_table in fks.items():\n",
    "            if fk_col not in df_copy.columns:\n",
    "                continue\n",
    "            if ref_table not in pk_maps:\n",
    "                continue\n",
    "            fk_map = pk_maps[ref_table]\n",
    "            # Для значений внешнего ключа, которые есть в fk_map, меняем, а если нет — оставляем как есть\n",
    "            df_copy[fk_col] = df_copy[fk_col].map(lambda x: fk_map.get(x, x))\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    return dfs_new\n",
    "\n",
    "def merge_dfs(dfs1, dfs2):\n",
    "    \"\"\"\n",
    "    Объединяем две словаря таблиц (dataframe) по ключу (названию таблиц)\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    all_tables = set(dfs1.keys()).union(dfs2.keys())\n",
    "    for table in all_tables:\n",
    "        df1 = dfs1.get(table)\n",
    "        df2 = dfs2.get(table)\n",
    "        if df1 is not None and df2 is not None:\n",
    "            merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "        elif df1 is not None:\n",
    "            merged_df = df1.copy()\n",
    "        else:\n",
    "            merged_df = df2.copy()\n",
    "        merged[table] = merged_df\n",
    "    return merged\n",
    "\n",
    "def replace_base64_with_autoinc(dfs, pk_fk_info):\n",
    "    \"\"\"\n",
    "    Заменяет base64 ключи в объединённом наборе dfs на autoincrement int ключи,\n",
    "    поправив внешние ключи.\n",
    "\n",
    "    Возвращает новый набор dfs.\n",
    "    \"\"\"\n",
    "    pk_maps = {}  # table_name -> { old_base64_key: new_int_key }\n",
    "\n",
    "    dfs_new = {}\n",
    "\n",
    "    # Сначала для каждой таблицы создаём маппинг base64 pk -> int pk\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        if pk_col is None:\n",
    "            dfs_new[table] = df.copy()\n",
    "            continue\n",
    "\n",
    "        old_pks = df[pk_col].tolist()\n",
    "        # Создаем инт айдишки, нам надо уникальные:\n",
    "        unique_old_pks = pd.Series(old_pks).unique()\n",
    "        pk_map = dict(zip(unique_old_pks, range(1, len(unique_old_pks) + 1)))\n",
    "        pk_maps[table] = pk_map\n",
    "\n",
    "        # Заменяем pk в df\n",
    "        df_copy = df.copy()\n",
    "        df_copy[pk_col] = df_copy[pk_col].map(pk_map)\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    # Обновляем foreign key\n",
    "    for table, df in dfs_new.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        fks = info.get('fks', {})\n",
    "        if not fks:\n",
    "            continue\n",
    "        df_copy = df.copy()\n",
    "        for fk_col, ref_table in fks.items():\n",
    "            if fk_col not in df_copy.columns:\n",
    "                continue\n",
    "            if ref_table not in pk_maps:\n",
    "                continue\n",
    "            fk_map = pk_maps[ref_table]\n",
    "            df_copy[fk_col] = df_copy[fk_col].map(lambda x: fk_map.get(x, x))\n",
    "        dfs_new[table] = df_copy\n",
    "\n",
    "    return dfs_new\n",
    "\n",
    "def write_dfs_to_sqlite(dfs, pk_fk_info, output_db_path):\n",
    "    \"\"\"\n",
    "    Записывает датафреймы в SQLite файл\n",
    "\n",
    "    При этом для таблиц с PK ставит INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    для других просто создаёт таблицы.\n",
    "\n",
    "    !! Внимание !! \n",
    "    Здесь упрощённая логика создания таблиц — для реальной сложной схемы \n",
    "    стоит использовать инспектор схемы или ORM\n",
    "\n",
    "    Принимает:\n",
    "      - dfs: dict table->DataFrame\n",
    "      - pk_fk_info: dict с info по key-ам\n",
    "      - output_db_path: файл для записи\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_db_path):\n",
    "        os.remove(output_db_path)\n",
    "\n",
    "    conn = sqlite3.connect(output_db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for table, df in dfs.items():\n",
    "        info = pk_fk_info.get(table, {})\n",
    "        pk_col = info.get('pk')\n",
    "        fks = info.get('fks', {})\n",
    "\n",
    "        # Формируем схему\n",
    "        columns = df.columns.tolist()\n",
    "        col_defs = []\n",
    "\n",
    "        for col in columns:\n",
    "            dtype = df[col].dtype\n",
    "            if pd.api.types.is_integer_dtype(dtype):\n",
    "                col_type = 'INTEGER'\n",
    "            elif pd.api.types.is_float_dtype(dtype):\n",
    "                col_type = 'REAL'\n",
    "            else:\n",
    "                col_type = 'TEXT'\n",
    "\n",
    "            if pk_col == col:\n",
    "                col_def = f\"{col} INTEGER PRIMARY KEY AUTOINCREMENT\"\n",
    "            else:\n",
    "                col_def = col + ' ' + col_type\n",
    "            col_defs.append(col_def)\n",
    "\n",
    "        sql_create = f\"CREATE TABLE {table} ({', '.join(col_defs)}\"\n",
    "\n",
    "        # Добавляем внешние ключи (если есть)\n",
    "        if fks:\n",
    "            fk_defs = []\n",
    "            for fk_col, ref_table in fks.items():\n",
    "                pk_ref_col = pk_fk_info[ref_table]['pk']\n",
    "                fk_defs.append(f\"FOREIGN KEY ({fk_col}) REFERENCES {ref_table}({pk_ref_col})\")\n",
    "            if fk_defs:\n",
    "                sql_create += ', ' + ', '.join(fk_defs)\n",
    "\n",
    "        sql_create += ');'\n",
    "\n",
    "        cursor.execute(sql_create)\n",
    "\n",
    "        # Вставляем данные\n",
    "        placeholders = ', '.join(['?' for _ in columns])\n",
    "        insert_sql = f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "        values = df.where(pd.notnull(df), None).values.tolist()\n",
    "        cursor.executemany(insert_sql, values)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def merge_sqlite_dbs_with_key_replacement(db1_path, db2_path, output_db_path, pk_fk_info):\n",
    "    \"\"\"\n",
    "    Основная функция:\n",
    "    1) Загружает обе базы в DF\n",
    "    2) Заменяет PK в обеих базах на 256бит base64 ключи с обновлением FK\n",
    "    3) Объединяет DF по таблицам\n",
    "    4) Заменяет base64 ключи на int PK с обновлением FK\n",
    "    5) Записывает в SQLite\n",
    "\n",
    "    pk_fk_info: структура с info по первичным и внешним ключам\n",
    "\n",
    "    Пример pk_fk_info:\n",
    "    {\n",
    "       'authors': {\n",
    "           'pk': 'author_id',\n",
    "           'fks': {}\n",
    "       },\n",
    "       'books': {\n",
    "           'pk': 'book_id',\n",
    "           'fks': {'author_id': 'authors'}\n",
    "       }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Загрузка баз...\")\n",
    "    dfs1 = load_dbs_as_dfs(db1_path)\n",
    "    dfs2 = load_dbs_as_dfs(db2_path)\n",
    "\n",
    "    print(\"Заменяем PK на base64 в первой базе...\")\n",
    "    dfs1_b64 = replace_keys_base64(dfs1, pk_fk_info)\n",
    "\n",
    "    print(\"Заменяем PK на base64 во второй базе...\")\n",
    "    dfs2_b64 = replace_keys_base64(dfs2, pk_fk_info)\n",
    "\n",
    "    print(\"Объединяем датафреймы...\")\n",
    "    merged_dfs = merge_dfs(dfs1_b64, dfs2_b64)\n",
    "\n",
    "    print(\"Удаляем дубликаты с корректировкой foreign keys...\")\n",
    "    merged_deduped_dfs = deduplicate_merged_dfs(merged_dfs, pk_fk_info)\n",
    "\n",
    "    print(\"Заменяем base64 PK на int PK с автоинкрементом...\")\n",
    "    final_dfs = replace_base64_with_autoinc(merged_deduped_dfs, pk_fk_info)\n",
    "\n",
    "    print(\"Записываем в SQLite...\")\n",
    "    write_dfs_to_sqlite(final_dfs, pk_fk_info, output_db_path)\n",
    "\n",
    "    print(f\"Обработка завершена. Итоговая база: {output_db_path}\")\n",
    "\n",
    "\n",
    "# --- ПРИМЕР ВЫЗОВА ---\n",
    "# Необходимо задать pk_fk_info для вашей схемы БД (пример ниже)\n",
    "# pk_fk_info = {\n",
    "#   'table1': {'pk': 'id', 'fks': {'parent_id': 'table1'}},\n",
    "#   'table2': {'pk': 'id', 'fks': {'table1_id': 'table1'}},\n",
    "# }\n",
    "\n",
    "# merge_sqlite_dbs_with_key_replacement('db1.sqlite', 'db2.sqlite', 'merged.sqlite', pk_fk_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea4dd6f-a9dd-47b6-83b3-b4e6b6438722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_fk_info = {\n",
    "    'animal_docs': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'animal': 'animals_simple',\n",
    "            'cattle': 'cattle'\n",
    "        }\n",
    "    },\n",
    "    'animals_simple': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'cattle': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'city': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_settlement': 'settlement'\n",
    "        }\n",
    "    },\n",
    "    'conducted_tests': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'animal': 'cattle'\n",
    "        }\n",
    "    },\n",
    "    'household': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_city': 'city'\n",
    "        }\n",
    "    },\n",
    "    'report_entries': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'belongs_to_report': 'reports',\n",
    "            'household': 'household'\n",
    "        }\n",
    "    },\n",
    "    'reports': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'city': 'city'\n",
    "        }\n",
    "    },\n",
    "    'settlement': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2ce44b-8d36-4cfc-a7cd-3065bcaed748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка баз...\n",
      "Заменяем PK на base64 в первой базе...\n",
      "Заменяем PK на base64 во второй базе...\n",
      "Объединяем датафреймы...\n",
      "Удаляем дубликаты с корректировкой foreign keys...\n",
      "Заменяем base64 PK на int PK с автоинкрементом...\n",
      "Записываем в SQLite...\n",
      "Обработка завершена. Итоговая база: result_database.sqlite3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kingfish\\AppData\\Local\\Temp\\ipykernel_13788\\1338484629.py:36: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_vals, group_df in grouped:\n"
     ]
    }
   ],
   "source": [
    "# PRIMER BLIN\n",
    "pk_fk_info = {\n",
    "    'gay_clubs': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {}\n",
    "    },\n",
    "    'porno': {\n",
    "        'pk': 'pk',\n",
    "        'fks': {\n",
    "            'gay_club': 'gay_clubs'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "merge_sqlite_dbs_with_key_replacement(db1_path, db2_path, output_db_path, pk_fk_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "090ed67e-5059-4981-aed9-dbf010387545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# ДАЛЬШЕ ИДЁТ АВТОГЕНЕРАТОР pk_fk_info ПРИ УСЛОВИИ ЧТО ИМЕНА В ТАБЛИЦЕ ПОДЧИНЯЮТСЯ ПРАВИЛАМ АЛЬФИРЫ МЕНЛИГУЛОВНЫ ИЛИ+- С ПРЕФИКСОМ И ПОСТФИКСОМ\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831f6a3f-016e-449b-a8b8-f90d9a3ab85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import json\n",
    "\n",
    "def generate_pk_fk_info_from_db(db_path, output_path, fk_prefix, fk_postfix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Генерирует pk_fk_info из схемы SQLite базы данных,\n",
    "    где foreign key колонки по шаблону: prefix + имя_таблицы_на_которую_ссылается + postfix\n",
    "\n",
    "    Args:\n",
    "        db_path (str): путь к SQLite базе\n",
    "        output_path (str): путь к файлу для сохранения pk_fk_info\n",
    "        fk_prefix (str): префикс для FK-колонок\n",
    "        fk_postfix (str): постфикс для FK-колонок\n",
    "\n",
    "    Результат:\n",
    "        Распечатывает pk_fk_info и сохраняет в файл output_path\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Получаем все таблицы\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    pk_fk_info = {}\n",
    "\n",
    "    for table in tables:\n",
    "        # Получаем info о колонках - pragma table_info\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns_info = cursor.fetchall()  # (cid, name, type, notnull, dflt_value, pk)\n",
    "\n",
    "        # Определяем PK - предполагаем, что PK одна колонка с pk>0\n",
    "        pk_cols = [col[1] for col in columns_info if col[5] > 0]\n",
    "        pk = pk_cols[0] if pk_cols else None  # None, если нет PK\n",
    "\n",
    "        # Подготовим словарь для FK колонок\n",
    "        fk_dict = {}\n",
    "\n",
    "        # Теперь пытаемся понять, на какие таблицы ссылаются FK колонки по шаблону имени:\n",
    "        # FK-колонки имеют имя: fk_prefix + referenced_table + fk_postfix\n",
    "\n",
    "        # Перебираем все таблицы как возможные referenced_table\n",
    "        for ref_table in tables:\n",
    "            expected_fk_col = f\"{fk_prefix}{ref_table}{fk_postfix}\"\n",
    "            # Если такая колонка есть в текущей таблице - регистрируем FK\n",
    "            col_names = [col[1] for col in columns_info]\n",
    "            if expected_fk_col in col_names:\n",
    "                fk_dict[expected_fk_col] = ref_table\n",
    "\n",
    "        pk_fk_info[table] = {\n",
    "            'pk': pk,\n",
    "            'fks': fk_dict\n",
    "        }\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Выведем результат красиво (JSON-подобном формате)\n",
    "    import pprint\n",
    "    print(\"Сгенерированный pk_fk_info:\")\n",
    "    pprint.pprint(pk_fk_info)\n",
    "    output_path = output_path + \" pk_fk_info.txt\"\n",
    "    # Запишем в файл\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        # Можно сохранить красиво в json с отступами\n",
    "        import json\n",
    "        json.dump(pk_fk_info, f_out, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\npk_fk_info сохранён в файл: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9957d649-a832-4733-8690-b8539dfd8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный pk_fk_info:\n",
      "{'gay_clubs': {'fks': {}, 'pk': 'pk'},\n",
      " 'porno': {'fks': {'gay_clubs': 'gay_clubs'}, 'pk': 'pk'}}\n",
      "\n",
      "pk_fk_info сохранён в файл: ./nichego/baza1.sqlite3 pk_fk_info.txt\n"
     ]
    }
   ],
   "source": [
    "db_file = db1_path\n",
    "output_file = db_file\n",
    "\n",
    "generate_pk_fk_info_from_db(db_file, output_file, fk_prefix='', fk_postfix='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e5835-8bb9-46f1-bf62-9b1ad22e6a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
